# Image-Captioning-VLM

**Visionâ€“Language Model (VLM) for Automatic Image Captioning using BLIP-2**

> Generate rich, human-like textual descriptions from images using a state-of-the-art Visionâ€“Language Model.

---

## ğŸ“Œ Overview

**Image-Captioning-VLM** is a deep learningâ€“based project that leverages **Salesforceâ€™s BLIP-2 Visionâ€“Language Model** to automatically generate descriptive captions for images.

This project demonstrates how **vision encoders + large language models (LLMs)** can be combined to perform multimodal reasoning â€” a core concept behind modern AI systems such as GPT-4V, Gemini, and Claude Vision.

---

## âœ¨ Key Features

* ğŸ” **Visionâ€“Language Understanding (VLM)**
* ğŸ§  Powered by **BLIP-2 (Bootstrapped Language-Image Pretraining)**
* ğŸ–¼ï¸ Supports **JPEG, PNG, JPG** image formats
* ğŸ“ Outputs **natural language captions**
* ğŸš€ Runs efficiently on **Google Colab (T4 GPU recommended)**
* ğŸ” Uses **Hugging Face Inference API**
* ğŸ““ Fully implemented in **Jupyter Notebook**
* ğŸ¯ Beginner-friendly, research-ready structure

---

## ğŸ§  Model Architecture 


![Cool cat](https://github.com/user-attachments/assets/abc7358d-4412-4ff3-a7e8-02a977ed6124)

### Model Used

* **BLIP-2** by Salesforce
  (`Salesforce/blip2-flan-t5-xl` or similar)

BLIP-2 bridges vision and language by:

* Extracting visual features using a frozen vision encoder
* Translating visual embeddings into language-compatible representations
* Generating captions using an LLM without retraining the full model

---

## ğŸ“‚ Repository Structure

```
Image-Captioning-VLM/
â”‚
â”œâ”€â”€ Image_Captioning.ipynb   # Main implementation notebook
â”œâ”€â”€ README.md                # Project documentation
â”œâ”€â”€ LICENSE                  # MIT License
â””â”€â”€ .gitignore               # Ignored files
```

---

## ğŸ› ï¸ Tech Stack

| Component | Technology                 |
| --------- | -------------------------- |
| Language  | Python                     |
| Framework | PyTorch                    |
| Model     | Salesforce BLIP-2          |
| Platform  | Google Colab               |
| API       | Hugging Face Inference API |
| Notebook  | Jupyter                    |

---

## ğŸš€ Getting Started

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/samnaveenkumaroff/Image-Captioning-VLM.git
cd Image-Captioning-VLM
```

---

### 2ï¸âƒ£ Open in Google Colab (Recommended)

1. Go to **Google Colab**
2. Click **File â†’ Open Notebook**
3. Select **GitHub**
4. Paste this repository URL:

   ```
   https://github.com/samnaveenkumaroff/Image-Captioning-VLM
   ```
5. Open `Image_Captioning.ipynb`

---

### 3ï¸âƒ£ Enable GPU 

For best performance:

```
Runtime â†’ Change runtime type â†’ Hardware Accelerator â†’ GPU
```

âœ… **Recommended GPU**: `T4`

---

## ğŸ” Getting a Hugging Face API Token

BLIP-2 requires access via Hugging Face.

### Step-by-Step Guide

1. Visit ğŸ‘‰ [https://huggingface.co](https://huggingface.co)
2. Sign up / Log in
3. Go to **Settings â†’ Access Tokens**
4. Click **New Token**
5. Choose:

   * Name: `Image-Captioning`
   * Role: **Read**
6. Copy the token

---

### ğŸ”‘ Add Token in Google Colab

```python
from huggingface_hub import login
login("YOUR_HUGGING_FACE_API_TOKEN")
```

âš ï¸ **Never commit your token to GitHub**

---

## ğŸ–¼ï¸ Supported Image Formats

* `.jpg`
* `.jpeg`
* `.png`

You can upload images directly in Colab or load them via file paths.

---

## ğŸ§ª Example Output

<img width="804" height="806" alt="image" src="https://github.com/user-attachments/assets/ff8e0d10-858e-41e6-b697-cdaab2fd60e1" />


---

## ğŸ¯ Use Cases

* ğŸ“¸ Automated image annotation
* â™¿ Accessibility (screen readers)
* ğŸ§¾ Media asset tagging
* ğŸ¤– Multimodal AI research
* ğŸ“Š Dataset labeling
* ğŸ›ï¸ E-commerce product descriptions

---

## ğŸ“ˆ Future Improvements

* ğŸ”„ Batch image captioning
* ğŸŒ Multilingual captioning
* ğŸ§  Fine-tuning on custom datasets
* ğŸ¥ Video captioning extension

---

## ğŸ“œ License

This project is licensed under the **MIT License**
You are free to use, modify, and distribute with attribution.

---

## ğŸ‘¨â€ğŸ’» Author

**Sam Naveenkumar V**

> *AI Research | Vision-Language Models | Generative AI*

Made with â¤ï¸ .

---

## ğŸŒ Connect With Me

ğŸ“§ **Email**:
[samnaveenkumaroff@gmail.com](mailto:samnaveenkumaroff@gmail.com)

ğŸ’¼ **LinkedIn**:
[https://www.linkedin.com/in/sam-naveenkumar-v/](https://www.linkedin.com/in/samnaveenkumaroff/)

ğŸ™ **GitHub**:
[https://github.com/samnaveenkumaroff](https://github.com/samnaveenkumaroff)

âœï¸ **Medium**:
[https://medium.com/@samnaveenkumaroff](https://medium.com/@samnaveenkumaroff)

ğŸ“¸ **Portfolio / Projects**:
[https://github.com/samnaveenkumaroff?tab=repositories](https://github.com/samnaveenkumaroff/)

---

## â­ Support

If you found this project helpful:

* â­ Star the repository
* ğŸ´ Fork it
* ğŸ› Open issues
* ğŸ¤ Contribute improvements

---

> **â€œVision + Language is the foundation of general intelligence.â€**

ğŸš€ Happy Coding & Research!
